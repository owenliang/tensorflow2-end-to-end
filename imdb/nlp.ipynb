{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>3453_3</td>\n",
       "      <td>0</td>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>5064_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>10905_3</td>\n",
       "      <td>0</td>\n",
       "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>10194_3</td>\n",
       "      <td>0</td>\n",
       "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>8478_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review\n",
       "0       5814_8          1  With all this stuff going down at the moment w...\n",
       "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3       3630_4          0  It must be assumed that those who praised this...\n",
       "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
       "...        ...        ...                                                ...\n",
       "24995   3453_3          0  It seems like more consideration has gone into...\n",
       "24996   5064_1          0  I don't believe they made this film. Completel...\n",
       "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
       "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...\n",
       "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./labeledTrainData.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.15, random_state=41)\n",
    "train, val = train_test_split(train, test_size=0.15, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分词，保留词频最高的5000个词，用数字编号，得到词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文分词，保留频次最高的5000个词\n",
    "vocab_size = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train['review'])\n",
    "\n",
    "# 补充：如果中文则需要自己对样本先jieba分词，然后通过tokenizer.fit_on_sequences输入分词列表即可，后续都是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_words', 'filters', 'lower', 'split', 'char_level', 'oov_token', 'document_count', 'word_counts', 'word_docs', 'index_docs', 'index_word', 'word_index'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看词表\n",
    "conf = tokenizer.get_config()\n",
    "conf.keys() \n",
    "# index_word: 词ID -> 词\n",
    "# word_index: 词 -> 词ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词表可以导出json，工程侧可以加载词表进行文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存词表为Json字符串\n",
    "json_conf = tokenizer.to_json()\n",
    "\n",
    "# 从json字符串加载词表\n",
    "tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(json_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据词表将样本转换为词id列表\n",
    "train_review = tokenizer.texts_to_sequences(train['review'])\n",
    "test_review = tokenizer.texts_to_sequences(test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个样本仅保留首先出现的256个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每条评论只保留256个词\n",
    "train_review = tf.keras.preprocessing.sequence.pad_sequences(train_review, value=0, padding='post',maxlen=256)\n",
    "test_review = tf.keras.preprocessing.sequence.pad_sequences(test_review, value=0, padding='post',maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理成dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(({'review': train_review}, train['sentiment'])).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(({'review': test_review}, test['sentiment'])).batch(32)\n",
    "#for features, label in train_ds:\n",
    "#    print(features, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征预处理（embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征预处理\n",
    "review_cate_col = tf.feature_column.categorical_column_with_vocabulary_list('review', range(vocab_size+1),default_value=0)\n",
    "review_embedding_col = tf.feature_column.embedding_column(review_cate_col, dimension=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.DenseFeatures([review_embedding_col]), # 预处理层\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'), # sigmoid激活到0~1以拟合目标值\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # 用交叉熵算损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.5882 - accuracy: 0.7086\n",
      "Epoch 2/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.2975 - accuracy: 0.8820\n",
      "Epoch 3/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.2199 - accuracy: 0.9158\n",
      "Epoch 4/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.1772 - accuracy: 0.9355\n",
      "Epoch 5/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.1464 - accuracy: 0.9495\n",
      "Epoch 6/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.1220 - accuracy: 0.9605\n",
      "Epoch 7/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9684\n",
      "Epoch 8/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0841 - accuracy: 0.9764\n",
      "Epoch 9/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9826\n",
      "Epoch 10/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0560 - accuracy: 0.9869\n",
      "Epoch 11/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0453 - accuracy: 0.9900\n",
      "Epoch 12/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0371 - accuracy: 0.9931\n",
      "Epoch 13/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0319 - accuracy: 0.9942\n",
      "Epoch 14/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0300 - accuracy: 0.9944\n",
      "Epoch 15/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0338 - accuracy: 0.9914\n",
      "Epoch 16/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0468 - accuracy: 0.9860\n",
      "Epoch 17/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0459 - accuracy: 0.9849\n",
      "Epoch 18/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0365 - accuracy: 0.9887\n",
      "Epoch 19/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0261 - accuracy: 0.9926\n",
      "Epoch 20/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0183 - accuracy: 0.9961\n",
      "Epoch 21/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0148 - accuracy: 0.9973\n",
      "Epoch 22/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0120 - accuracy: 0.9981\n",
      "Epoch 23/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0100 - accuracy: 0.9984\n",
      "Epoch 24/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0082 - accuracy: 0.9990\n",
      "Epoch 25/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0093 - accuracy: 0.9980\n",
      "Epoch 26/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0102 - accuracy: 0.9976\n",
      "Epoch 27/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0100 - accuracy: 0.9972\n",
      "Epoch 28/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 29/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0109 - accuracy: 0.9968\n",
      "Epoch 30/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0121 - accuracy: 0.9963\n",
      "Epoch 31/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0070 - accuracy: 0.9981\n",
      "Epoch 32/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0044 - accuracy: 0.9991\n",
      "Epoch 33/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 34/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0032 - accuracy: 0.9996\n",
      "Epoch 35/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 36/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 37/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 38/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0090 - accuracy: 0.9969\n",
      "Epoch 39/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0040 - accuracy: 0.9988\n",
      "Epoch 40/40\n",
      "565/565 [==============================] - 2s 3ms/step - loss: 0.0016 - accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe4ecd272d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, batch_size=512, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 - 0s - loss: 1.3793 - accuracy: 0.8605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3792898654937744, 0.8605333566665649]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
